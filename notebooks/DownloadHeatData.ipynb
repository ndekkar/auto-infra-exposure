{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f34f747-5eea-4e6f-b290-0d6979703f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2010.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2011.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2012.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2013.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2014.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2015.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2016.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2017.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2018.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2019.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2020.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2021.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2022.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2023.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_hourly_tjk_2024.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2010.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2011.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2012.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2013.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2014.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2015.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2016.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2017.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2018.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2019.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2020.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2021.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2022.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2023.nc\n",
      "[skip] already exists: _tmp_era5_tjk/era5_t2m_dailyTmax_tjk_2024.nc\n",
      "[concat] assembling all yearly daily files -> final NetCDF\n",
      "✅ Done. Final file ready for your pipeline:\n",
      "/Users/nassimdekkar/Documents/GitHub/RI-Infra-exposure/data/heat_tajikistan_2010_2024.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cdsapi\n",
    "import xarray as xr\n",
    "\n",
    "# -------------------\n",
    "# CONFIG (edit here)\n",
    "# -------------------\n",
    "YEARS = list(range(2010, 2025))  # Year from 2010 to 2025 can be changed\n",
    "# Tajikistan bbox [North, West, South, East] for CDS\n",
    "AREA_TJK = [41.1, 67.3, 36.5, 75.2]  \n",
    "OUT_DIR = Path(\"../data\")            # where the final NetCDF will go\n",
    "FINAL_NC = OUT_DIR / \"heat_tajikistan_2010_2024.nc\"\n",
    "TMP_DIR = Path(\"./_tmp_era5_tjk\")    # temp download/processing folder\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------\n",
    "# 1) DOWNLOAD PER YEAR\n",
    "# -------------------\n",
    "c = cdsapi.Client()\n",
    "hourly_files = []\n",
    "\n",
    "for y in YEARS:\n",
    "    hourly_nc = TMP_DIR / f\"era5_t2m_hourly_tjk_{y}.nc\"\n",
    "    if hourly_nc.exists():\n",
    "        print(f\"[skip] already exists: {hourly_nc}\")\n",
    "        hourly_files.append(hourly_nc)\n",
    "        continue\n",
    "    print(f\"[download] ERA5 hourly t2m for {y}\")\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-single-levels\",\n",
    "        {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": \"2m_temperature\",\n",
    "            \"year\": str(y),\n",
    "            \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
    "            \"day\": [f\"{d:02d}\" for d in range(1, 32)],\n",
    "            \"time\": [f\"{h:02d}:00\" for h in range(24)],\n",
    "            \"area\": AREA_TJK,          # [N, W, S, E]\n",
    "            \"format\": \"netcdf\",\n",
    "        },\n",
    "        str(hourly_nc),\n",
    "    )\n",
    "    hourly_files.append(hourly_nc)\n",
    "\n",
    "# -------------------\n",
    "# 2) PROCESS PER YEAR -> DAILY TMAX (°C)\n",
    "#  \n",
    "# -------------------\n",
    "daily_files = []\n",
    "for hourly_nc in sorted(hourly_files):\n",
    "    y = hourly_nc.stem.split(\"_\")[-1]  # crude parse of year\n",
    "    daily_nc = TMP_DIR / f\"era5_t2m_dailyTmax_tjk_{y}.nc\"\n",
    "    if daily_nc.exists():\n",
    "        print(f\"[skip] already exists: {daily_nc}\")\n",
    "        daily_files.append(daily_nc)\n",
    "        continue\n",
    "\n",
    "    print(f\"[process] {hourly_nc.name} -> daily Tmax (°C)\")\n",
    "    ds_hour = xr.open_dataset(hourly_nc, chunks={\"time\": 240})  # dask-friendly\n",
    "    da = ds_hour[\"t2m\"] - 273.15                                # Kelvin -> °C\n",
    "    da.name = \"t2m\"\n",
    "    da.attrs[\"units\"] = \"degC\"\n",
    "    # daily maximum over the 24 hours\n",
    "    time_dim = \"time\" if \"time\" in da.dims else \"valid_time\"\n",
    "    da_daily_tmax = da.resample({time_dim: \"1D\"}).max(skipna=True)\n",
    "    da_daily_tmax.to_dataset(name=\"t2m\").to_netcdf(daily_nc)\n",
    "    daily_files.append(daily_nc)\n",
    "    ds_hour.close()\n",
    "\n",
    "# -------------------\n",
    "# 3) CONCAT ALL YEARS -> SINGLE DAILY NETCDF\n",
    "# -------------------\n",
    "print(\"[concat] assembling all yearly daily files -> final NetCDF\")\n",
    "# combine by coordinates handles contiguous time properly\n",
    "ds_all = xr.open_mfdataset([str(f) for f in sorted(daily_files)],\n",
    "                           combine=\"by_coords\", parallel=True)\n",
    "# optional: ensure nice attrs\n",
    "ds_all[\"t2m\"].attrs[\"long_name\"] = \"Daily maximum 2m air temperature\"\n",
    "ds_all[\"t2m\"].attrs[\"units\"] = \"degC\"\n",
    "\n",
    "# write final file\n",
    "ds_all.to_netcdf(FINAL_NC)\n",
    "ds_all.close()\n",
    "\n",
    "print(f\"✅ Done. Final file ready for your pipeline:\\n{FINAL_NC.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdffa7a-e1bc-48a3-b0f9-64af7e038e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc21b03f-8a6b-4dc5-aa91-8273afea4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "#  In case of the kernel is full\n",
    "# -------------------\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85c31e-23fb-4c97-8302-18771cb40b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
